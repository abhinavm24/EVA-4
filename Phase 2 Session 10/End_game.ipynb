{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "End game.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karthikvijayakumar/EVA/blob/master/Phase%202%20Session%2010/End_game.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpgQqPDOuOTp",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGJBR6vXuOT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from PIL import Image, ImageOps, ImageDraw\n",
        "from collections import deque\n",
        "\n",
        "import gym\n",
        "from gym import error, spaces, utils, wrappers\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwL2RxKzuOUr",
        "colab_type": "text"
      },
      "source": [
        "# Custom gym environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oX4qi5q7uOUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CityMap(gym.Env):\n",
        "    metadata = {'render.modes': ['human', 'rgb_array']}\n",
        "    reward_range = (-float('inf'), float('inf'))\n",
        "    spec = None\n",
        "    \n",
        "    observation_window_size = 80\n",
        "    # observation_window_size is the side length of the square surrounding the car\n",
        "    # The car would see (observation_window_size/2) ahead and behind and (observation_window_size/2) to the left and right\n",
        "    \n",
        "    max_action = np.float64(1.0)\n",
        "\n",
        "    max_turn_radians = np.pi/36.0\n",
        "    # pi/6 radians = 180/36 = 5 degrees\n",
        "    # The car can turn 5 degrees to the left or right    \n",
        "    \n",
        "    distance_threshold_done = 30\n",
        "    # Distance to target to reach before considering the episode done\n",
        "    \n",
        "    goal_circle_radius = int(distance_threshold_done/2)\n",
        "\n",
        "    #Max steps before we call the episode done\n",
        "    max_episode_steps = 2500\n",
        "    \n",
        "    def __init__(self, citymap, roadmask, car_image, render_pov = 'map'):\n",
        "        self.action_space = spaces.Box(low = np.float64(-self.max_action), high = np.float64(self.max_action), shape = (1,) ) \n",
        "        # Action space is how many degrees to turn the car in radians\n",
        "        \n",
        "        self.observation_space = spaces.Box(low = 0, high = 255, shape = (self.observation_window_size, self.observation_window_size) ) \n",
        "        # All combinations of white and black pixels of observation_window_size x observation_window_size\n",
        "        \n",
        "        self.state = None\n",
        "        \n",
        "        self.citymap = citymap.copy()\n",
        "        self.roadmask = roadmask.copy()\n",
        "        self.car_image = car_image.copy()\n",
        "        self.render_pov = render_pov\n",
        "        \n",
        "        #Find size of the roadmask for reference later\n",
        "        self.roadmask_size_x, self.roadmask_size_y = self.roadmask.getbbox()[2:4]\n",
        "        \n",
        "        # Pad the road mask image to allow for rotations\n",
        "        # Amount of padding required = ( diagonal length of the observation window )/2\n",
        "        self.padding_size = int(self.observation_window_size/np.sqrt(2))\n",
        "        padding = ( self.padding_size, self.padding_size, self.padding_size, self.padding_size )\n",
        "        self.roadmaskpadded = ImageOps.expand( self.roadmask, padding, fill = 255 ) # Pad and fill with sand\n",
        "        \n",
        "        #Set goal point\n",
        "        self.goal_x = 1154\n",
        "        self.goal_y = 158\n",
        "        \n",
        "        self.car_pos_x = 0\n",
        "        self.car_pos_y = 0\n",
        "\n",
        "        self.num_steps = 0\n",
        "        \n",
        "        self.reset()\n",
        "\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "        \n",
        "        Returns:\n",
        "            ( next_state, reward, done, info )\n",
        "        \n",
        "    \"\"\"\n",
        "    def step(self, action):\n",
        "\n",
        "        # Type check to ensure we get a scalar\n",
        "        assert ((type(action) == np.float32) | (type(action) == np.float64)), \"Input type should be a float32 or float64\"\n",
        "        \n",
        "        # Things to compute\n",
        "        # 1. Next position         \n",
        "        # 2. Reward on moving to next position\n",
        "        # 3. Orientation of car towards goal\n",
        "        # 4. Combine Screen grab from next position and orientation to produce the next state\n",
        "        # 5. Update number of steps taken\n",
        "        # 6. Is the episode done        \n",
        "        # 7. Any info to pass on to the agent\n",
        "\n",
        "        # 1. Next position\n",
        "        # From (pos_x, pos_y) we move forward with 'speed' steps in the direction 'angle+action*max_turn_radians'\n",
        "        # The action given by the agent is from -1 to 1. The env maps the action to degrees of turn\n",
        "        # New angle of car\n",
        "        self.car_angle = self.car_angle + (action*self.max_turn_radians)\n",
        "        if(self.car_angle < 0):\n",
        "            self.car_angle = (2*np.pi) + self.car_angle\n",
        "        elif(self.car_angle > (2*np.pi)):\n",
        "            self.car_angle = self.car_angle - (2*np.pi)\n",
        "        \n",
        "        # Car speed depends on whether we are riding on sand or not\n",
        "        speed = 5 if self.roadmask.getpixel(( self.car_pos_x, self.car_pos_y )) == 0 else 2\n",
        "        \n",
        "        displacement_x = speed * np.sin( self.car_angle )\n",
        "        displacement_y = -1 * speed * np.cos( self.car_angle )\n",
        "        # Displacement y is negative since the top of the frame is y=0\n",
        "        # Hence if the car is pointing upwards ( oriented at 0 degrees ) then the y values would decrease\n",
        "        \n",
        "        old_car_pos_x = self.car_pos_x\n",
        "        old_car_pos_y = self.car_pos_y\n",
        "\n",
        "        self.car_pos_x = self.car_pos_x + displacement_x\n",
        "        self.car_pos_y = self.car_pos_y + displacement_y\n",
        "        \n",
        "        # Clip position to boundaries of the image\n",
        "        self.car_pos_x = np.clip(self.car_pos_x, 0, self.roadmask_size_x-1)\n",
        "        self.car_pos_y = np.clip(self.car_pos_y, 0, self.roadmask_size_y-1)\n",
        "        \n",
        "        # 2. Reward on moving to next position\n",
        "        \n",
        "        new_distance_from_goal = np.sqrt( (self.car_pos_x - self.goal_x)**2 + (self.car_pos_y - self.goal_y)**2 )\n",
        "        \n",
        "        pixel_value_at_car_pos = self.roadmask.getpixel((self.car_pos_x, self.car_pos_y))\n",
        "        \n",
        "        if( pixel_value_at_car_pos == 1 ):\n",
        "            #Currently on sand\n",
        "            # reward = -1\n",
        "            reward = -100 * ((new_distance_from_goal+100)/1650) # 1650 is the length of the diagonal of the image\n",
        "        elif( new_distance_from_goal > self.distance_from_goal ):\n",
        "            # reward = -0.2\n",
        "            reward = -10.2 * (new_distance_from_goal/1650)\n",
        "        elif ( new_distance_from_goal == self.distance_from_goal ):\n",
        "            # In one of the corners and driving into the corner\n",
        "            reward = -100\n",
        "        else:\n",
        "            # new_distance_from_goal < self.distance_from_goal\n",
        "            # reward = 0.1\n",
        "            reward = 0.2 * ((1650-new_distance_from_goal)/1650)\n",
        "\n",
        "        # Change reward on termination conditions\n",
        "\n",
        "        if( new_distance_from_goal < self.distance_threshold_done ):\n",
        "            # Give high +ve reward when it has reached the goal\n",
        "            reward = 100\n",
        "        elif( self.num_steps == self.max_episode_steps ):\n",
        "            # Give high -ve reward when the num steps has crossed max steps\n",
        "            reward = -1000\n",
        "        elif(\n",
        "            ( (old_car_pos_x-self.car_pos_x) == 0 and (self.car_pos_x == 0 or self.car_pos_x >= self.roadmask_size_x-1) ) or \n",
        "            ( (old_car_pos_y-self.car_pos_y) == 0 and (self.car_pos_y == 0 or self.car_pos_y >= self.roadmask_size_y-1) ) \n",
        "            ):\n",
        "            # Give high -ve reward when hitting a wall\n",
        "            reward = -1000\n",
        "\n",
        "        self.distance_from_goal = new_distance_from_goal\n",
        "\n",
        "        # 3. Compute orientation of car towards goal        \n",
        "\n",
        "        # 4. Combine screen grab from current position with orientation and distance to goal to form next state\n",
        "        next_state = ( self._extract_current_frame(), self._compute_orientation_towards_goal() , new_distance_from_goal )\n",
        "\n",
        "        # 5. Update number of steps taken\n",
        "        self.num_steps += 1\n",
        "        \n",
        "        # 6. Is the episode done?\n",
        "        \n",
        "        if( \n",
        "            new_distance_from_goal < self.distance_threshold_done  or \n",
        "            self.num_steps == self.max_episode_steps or\n",
        "            ( old_car_pos_x-self.car_pos_x == 0 and old_car_pos_y - self.car_pos_y == 0)\n",
        "            ):\n",
        "            # Either we have reached the target position or we have exceed the max steps for this episode\n",
        "            done = 1\n",
        "            self.reset()\n",
        "            next_state = (self._zero_screen_grab(),0,0)\n",
        "            # Return a zero screen grab, zero orientation and zero distance in case of termination\n",
        "            \n",
        "        else:\n",
        "            done = 0\n",
        "\n",
        "        # 7. Any info to pass on to the agent\n",
        "\n",
        "        # print(\n",
        "        #     \"x: \"+ str(self.car_pos_x.round(0)) + \n",
        "        #     \"; y: \" + str(self.car_pos_y.round(0)) + \n",
        "        #     \"; angle(deg): \" + str(np.round(self.car_angle*180/np.pi),2) + \n",
        "        #     \"; action: \" + str(np.round(action*180/np.pi,2)) +\n",
        "        #     \"; reward: \" + str(reward)\n",
        "        #     )\n",
        "        \n",
        "        return next_state, reward, done, {}\n",
        "\n",
        "    \"\"\"\n",
        "        Zero screen grab for episode termination conditions\n",
        "    \"\"\"\n",
        "    def _zero_screen_grab(self):\n",
        "        screen_grab = np.expand_dims( \n",
        "                np.expand_dims( \n",
        "                    np.zeros( int(self.observation_window_size/2)**2 ).reshape(\n",
        "                        ( int(self.observation_window_size/2), int(self.observation_window_size/2) )\n",
        "                        ),\n",
        "                    axis = 0 \n",
        "                ),\n",
        "                axis = 0 )\n",
        "        return screen_grab\n",
        "\n",
        "    \"\"\"\n",
        "        Definition of orientation:\n",
        "            With respect to the axes of car ( car's forward pointing upwards ), at how many degrees is the goal\n",
        "\n",
        "        We compute this in two steps:\n",
        "        Step 1: At what angle is the goal with respect to the vertical\n",
        "            Angle of goal wrt horizontal is tan_inverse( distance in y axis / distance in x axis )\n",
        "            Angle of goal wrt vertical is 90 + the above = 90 + tan_inverse( distance in y axis / distance in x axis )        \n",
        "        Step 2: Subtract the angle of the car from the above quantity to get angle relative to the car axes\n",
        "            Angle of goal wrt car = 90 + tan_inverse( distance in y axis / distance in x axis ) - car angle wrt vertical\n",
        "    \"\"\"\n",
        "    def _compute_orientation_towards_goal(self):\n",
        "        orientation = np.pi/2.0 + np.arctan2( self.goal_y - self.car_pos_y, self.goal_x - self.car_pos_x ) - self.car_angle\n",
        "        return orientation\n",
        "    \n",
        "    \"\"\"\n",
        "        Simple euclidean distance computation\n",
        "        Abstracted to a function to avoid rewriting in multiple places\n",
        "    \"\"\"\n",
        "    def _compute_distance_from_goal(self):\n",
        "        return np.sqrt( (self.car_pos_x - self.goal_x)**2 + (self.car_pos_y - self.goal_y)**2 ) \n",
        "\n",
        "    \"\"\"\n",
        "        Extracts the frame that the agent/car currently sees\n",
        "        With respect to the frame extracted the car is always pointing upward\n",
        "        Keeping the orientation fixed is key since else for the same scene( screen grab ), the car can be in different orientations \n",
        "        and hence should take different actions\n",
        "        \n",
        "        For example take the following case \n",
        "            Environment: A single straight road with the car in the middle on the road\n",
        "            Goal: Left end of the road and outside the visibility of the agent\n",
        "        \n",
        "            The car can be oriented left or right and the goal can be to the left or right\n",
        "            <<< NEED TO THINK OF A BETTER EXAMPLE >>>\n",
        "            \n",
        "        Parameters:\n",
        "            None\n",
        "        \n",
        "        Returns:\n",
        "            img - Numpy array of shape ( observation_window_size, observation_window_size )\n",
        "    \"\"\"\n",
        "    def _extract_current_frame(self):\n",
        "        # We know the current position of the car\n",
        "        # Step 1: Extract a square of size observation_window_size*sqrt(2) surrounding the car ( Call this rough cut )\n",
        "        # Step 2: Rotate the rough cut image around the center by angle of the car\n",
        "        # Step 3: Extract a square of size observation_window_size around the center\n",
        "        \n",
        "        \n",
        "        # Step 1: Extract a square of size observation_window_size*sqrt(2) surrounding the car ( Call this rough cut )\n",
        "        # We need to use the padded version of the road mask here\n",
        "        # Hence we add self.padding_size to the x,y position of the car\n",
        "        bounding_box_rough_cut = ( self.car_pos_x, self.car_pos_y, self.car_pos_x+(2*self.padding_size), self.car_pos_y+(2*self.padding_size) )\n",
        "        # print(\"Bounding box of rough cut: \" +str(bounding_box_rough_cut))\n",
        "\n",
        "        rough_cut = self.roadmaskpadded.crop(bounding_box_rough_cut)\n",
        "        \n",
        "        # Step 2: Rotate the rough cut image around the center by angle of the car\n",
        "        \n",
        "        rough_cut_rotated = rough_cut.rotate( self.car_angle * (180/np.pi) )\n",
        "        # PIL's rotate function:\n",
        "        #  - takes input in degrees ( 180 degrees = pi radians; x radians = x*(180/pi) degrees )\n",
        "        #  - by default rotates around the center of the image\n",
        "        #  - rotates anti-clockwise\n",
        "        \n",
        "        # Step 3: Extract a square of size observation_window_size around the center\n",
        "        # Center of the rough cut image is ( rough_cut_size/2, rough_cut_size/2 )\n",
        "        \n",
        "        bounding_box_current_frame = ( \n",
        "            self.padding_size - (self.observation_window_size/2), \n",
        "            self.padding_size - (self.observation_window_size/2), \n",
        "            self.padding_size + (self.observation_window_size/2), \n",
        "            self.padding_size + (self.observation_window_size/2)\n",
        "        )\n",
        "        \n",
        "        current_frame = rough_cut_rotated.crop(bounding_box_current_frame)\n",
        "        \n",
        "        # Scaling down the image to half the dimensions for optimising memory and simplifying input to agent\n",
        "        current_frame = current_frame.resize((int(self.observation_window_size/2), int(self.observation_window_size/2)) )\n",
        "\n",
        "        return np.expand_dims( np.expand_dims( np.asarray(current_frame)/255, axis = 0 ), axis = 0 )\n",
        "    \n",
        "    def reset(self):\n",
        "        #Randomly initialise the starting position and set velocity\n",
        "        self.car_pos_x = np.random.randint( 0, self.roadmask_size_x )\n",
        "        self.car_pos_y = np.random.randint( 0, self.roadmask_size_y )\n",
        "        # Car position is measured with respect to the road mask ( without padding ). (0,0) is top left\n",
        "        self.car_angle = np.random.default_rng().random() * np.pi * 2.0\n",
        "        # Initial angle ranges from 0 to 2*pi\n",
        "        # Angle measures rotation from vertical axis (i.e angle = 0 when car is heading upwards in the map)\n",
        "        \n",
        "        #Distance from goal\n",
        "        self.distance_from_goal = self._compute_distance_from_goal()\n",
        "\n",
        "        #Set num_steps to 0\n",
        "        self.num_steps = 0\n",
        "        \n",
        "        return (self._extract_current_frame(), self._compute_orientation_towards_goal(), self.distance_from_goal )\n",
        "\n",
        "\n",
        "    def render(self, mode='human', close=False):        \n",
        "        #Build image of map with goal and car overlaid\n",
        "        \n",
        "        #Create a copy of the map\n",
        "        map_copy = self.citymap.copy()\n",
        "        \n",
        "        #Draw a circle over the goal\n",
        "        draw = ImageDraw.Draw(map_copy)\n",
        "        draw.ellipse( \n",
        "            (self.goal_x - self.goal_circle_radius, \n",
        "             self.goal_y-self.goal_circle_radius, \n",
        "             self.goal_x+self.goal_circle_radius, \n",
        "             self.goal_y+self.goal_circle_radius\n",
        "            ), \n",
        "            fill = 'red', \n",
        "            outline = 'red', \n",
        "            width = 1 \n",
        "        )\n",
        "        del(draw)\n",
        "        \n",
        "        # Create a copy of the car and rotate it to the currrent orientation according to the env state\n",
        "        # Using 90 - curr_angle since the car image oriented horizontally while our angles are from the vertical\n",
        "        car_image_copy = self.car_image.copy().rotate( 360 - (self.car_angle*180/np.pi), expand = True )\n",
        "        car_size_x, car_size_y = car_image_copy.getbbox()[2:4] # The last 2 coordinates represent the size of the car\n",
        "        \n",
        "        #Overlay the car on the map ( copy )\n",
        "        map_copy.paste( car_image_copy, box = ( int(self.car_pos_x - (car_size_x/2)), int(self.car_pos_y - (car_size_y/2)) ) )\n",
        "        del(car_image_copy)\n",
        "        del(car_size_x)\n",
        "        del(car_size_y)       \n",
        "                \n",
        "        if mode == 'rgb_array':\n",
        "            if(self.render_pov == 'map'):            \n",
        "                return np.asarray(map_copy)\n",
        "            elif(self.render_pov == 'car'):\n",
        "                current_frame = Image.fromarray( self._extract_current_frame().squeeze(0).squeeze(0)*255 ).convert('RGB')\n",
        "                return np.asarray(current_frame)\n",
        "#         elif mode == 'human':\n",
        "#             if self.viewer is None:\n",
        "#                 self.viewer = rendering.SimpleImageViewer()\n",
        "# #             self.viewer.imshow(np.asarray(current_frame)*255)\n",
        "#             self.viewer.imshow(np.asarray(map_copy))\n",
        "#             return self.viewer.isopen\n",
        "    \n",
        "    def close(self):\n",
        "        pass\n",
        "        # if self.viewer is not None:\n",
        "        #     self.viewer.close()\n",
        "        #     print(self.viewer.isopen)\n",
        "        #     self.viewer = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsu91_DzuOVb",
        "colab_type": "text"
      },
      "source": [
        "# Model helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWQiadkRuOVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    sample_tuples = random.sample(self.storage, batch_size)\n",
        "    batch_screen, batch_orientation, batch_dist_goal, batch_next_screen, batch_next_orientation, batch_next_dist_goal, batch_actions, batch_rewards, batch_dones = tuple(zip(*sample_tuples))\n",
        "\n",
        "    return batch_screen, batch_orientation, batch_dist_goal, batch_next_screen, batch_next_orientation, batch_next_dist_goal, batch_actions, batch_rewards, batch_dones\n",
        "\n",
        "def evaluate_policy(policy, env, eval_episodes=3):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(obs)\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wkltbrJuOWg",
        "colab_type": "text"
      },
      "source": [
        "# Model classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sh14D50ouOWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 4, kernel_size=3,padding = 1, stride =2)\n",
        "        self.bn1 = nn.BatchNorm2d(4)\n",
        "        self.conv2 = nn.Conv2d(4,8, kernel_size=3, padding = 1, stride =2)\n",
        "        self.bn2 = nn.BatchNorm2d(8)\n",
        "        self.conv3 = nn.Conv2d(8,16, kernel_size=3, padding = 1, stride =2)\n",
        "        self.bn3 = nn.BatchNorm2d(16)\n",
        "        self.conv4 = nn.Conv2d(16, 32, kernel_size=3)\n",
        "        self.bn4 = nn.BatchNorm2d(32)\n",
        "        self.head = nn.Linear(32+2, action_dim)\n",
        "        # 32 channels from the conv layers, 2 inputs - orientation and distance from goal\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def forward(self, screen, orientation, dist_goal):\n",
        "        x = F.relu(self.bn1(self.conv1(screen)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = F.avg_pool2d( x, 3 )\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        #Concatenate orientation and distance to goal with the output of the convolutional layers\n",
        "        x = torch.cat([x, orientation, dist_goal], 1)\n",
        "\n",
        "        return self.max_action * torch.tanh(self.head(x.view( x.size(0),-1)))\n",
        "    \n",
        "class Critic(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Critic, self).__init__()\n",
        "    self.max_action = max_action\n",
        "        \n",
        "    # Defining the first Critic neural network\n",
        "    self.critic1_conv1 = nn.Conv2d(1, 4, kernel_size=3,padding = 1, stride =2)\n",
        "    self.critic1_bn1 = nn.BatchNorm2d(4)\n",
        "    self.critic1_conv2 = nn.Conv2d(4,8, kernel_size=3, padding = 1, stride =2)\n",
        "    self.critic1_bn2 = nn.BatchNorm2d(8)\n",
        "    self.critic1_conv3 = nn.Conv2d(8,16, kernel_size=3, padding = 1, stride =2)\n",
        "    self.critic1_bn3 = nn.BatchNorm2d(16)\n",
        "    self.critic1_conv4 = nn.Conv2d(16, 32, kernel_size=3)\n",
        "    self.critic1_bn4 = nn.BatchNorm2d(32)\n",
        "    self.critic1_head = nn.Linear(32+2+1, 1)\n",
        "    # 32 channels from the conv layers, 2 inputs - orientation and distance from goal, 1 input - action\n",
        "    # Critic gives out only 1 value hence the output dimension is one\n",
        "    # Critic also takes action as input which is a scalar, hence adding 1 to the output of the conv layers\n",
        "    \n",
        "    # Defining the second Critic neural network\n",
        "    self.critic2_conv1 = nn.Conv2d(1, 4, kernel_size=3,padding = 1, stride =2)\n",
        "    self.critic2_bn1 = nn.BatchNorm2d(4)\n",
        "    self.critic2_conv2 = nn.Conv2d(4,8, kernel_size=3, padding = 1, stride =2)\n",
        "    self.critic2_bn2 = nn.BatchNorm2d(8)\n",
        "    self.critic2_conv3 = nn.Conv2d(8,16, kernel_size=3, padding = 1, stride =2)\n",
        "    self.critic2_bn3 = nn.BatchNorm2d(16)\n",
        "    self.critic2_conv4 = nn.Conv2d(16, 32, kernel_size=3)\n",
        "    self.critic2_bn4 = nn.BatchNorm2d(32)\n",
        "    self.critic2_head = nn.Linear(32+2+1, 1)\n",
        "    # 32 channels from the conv layers, 2 inputs - orientation and distance from goal, 1 input - action\n",
        "    # Critic gives out only 1 value hence the output dimension is one\n",
        "    # Critic also takes action as input which is a scalar, hence adding 1 to the output of the conv layers\n",
        "\n",
        "  def forward(self, screen, orientation, dist_goal, action):\n",
        "    ###############\n",
        "    ## Critic 1\n",
        "    ###############\n",
        "    # Pass through convolutional layers\n",
        "    x1 = F.relu(self.critic1_bn1(self.critic1_conv1(screen)))\n",
        "    x1 = F.relu(self.critic1_bn2(self.critic1_conv2(x1)))\n",
        "    x1 = F.relu(self.critic1_bn3(self.critic1_conv3(x1)))\n",
        "    x1 = F.relu(self.critic1_bn4(self.critic1_conv4(x1)))\n",
        "    x1 = F.avg_pool2d( x1, 3 ) \n",
        "    x1 = x1.view(x1.size(0), -1)\n",
        "    \n",
        "    #Concatenate action with the output of the convolutional layers\n",
        "    x1 = torch.cat([x1, orientation, dist_goal, action], 1)\n",
        "    \n",
        "    #Pass through FC layer\n",
        "    x1 = self.critic1_head(x1)\n",
        "\n",
        "    ###############\n",
        "    # Critic 2\n",
        "    ###############\n",
        "    \n",
        "    #Pass through convolutional layers\n",
        "    x2 = F.relu(self.critic2_bn1(self.critic2_conv1(screen)))\n",
        "    x2 = F.relu(self.critic2_bn2(self.critic2_conv2(x2)))\n",
        "    x2 = F.relu(self.critic2_bn3(self.critic2_conv3(x2)))\n",
        "    x2 = F.relu(self.critic2_bn4(self.critic2_conv4(x2)))\n",
        "    x2 = F.avg_pool2d( x2, 3 ) \n",
        "    x2 = x2.view(x2.size(0), -1)\n",
        " \n",
        "    #Concatenate action with the output of the convolutional layers\n",
        "    x2 = torch.cat([x2, orientation, dist_goal, action], 1)\n",
        "    \n",
        "    #Pass through FC layer\n",
        "    x2 = self.critic2_head(x2)\n",
        "    \n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, screen, orientation, dist_goal, action):   \n",
        "    ###############\n",
        "    ## Critic 1\n",
        "    ###############\n",
        "    # Pass through convolutional layers\n",
        "    x1 = F.relu(self.critic1_bn1(self.critic1_conv1(screen)))\n",
        "    x1 = F.relu(self.critic1_bn2(self.critic1_conv2(x1)))\n",
        "    x1 = F.relu(self.critic1_bn3(self.critic1_conv3(x1)))\n",
        "    x1 = F.relu(self.critic1_bn4(self.critic1_conv4(x1)))\n",
        "    x1 = F.avg_pool2d( x1, 3 )\n",
        "    x1 = x1.view(x1.size(0), -1)\n",
        " \n",
        "    #Concatenate action with the output of the convolutional layers\n",
        "    x1 = torch.cat([x1, orientation, dist_goal, action], 1)\n",
        "    \n",
        "    #Pass through FC layer\n",
        "    x1 = self.critic1_head(x1)\n",
        "    \n",
        "    return x1\n",
        "\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action, device):\n",
        "    self.device = device\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(self.device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(self.device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim, max_action).to(self.device)\n",
        "    self.critic_target = Critic(state_dim, action_dim, max_action).to(self.device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  \"\"\"\n",
        "    Input params:\n",
        "      state - Tuple with 3 components ( screen, action, orientation )\n",
        "  \"\"\"\n",
        "  def select_action(self, state):\n",
        "    screen, orientation, dist_goal = state\n",
        "    screen = torch.Tensor(screen).to(self.device)\n",
        "    orientation = torch.Tensor([orientation]).view(-1,1).to(self.device) \n",
        "    dist_goal = torch.Tensor([dist_goal]).view(-1,1).to(self.device)\n",
        "    return self.actor(screen,orientation,dist_goal).detach().cpu().data.numpy().flatten()[0]\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    # print( \"Replay buffer size at beginning of call to train: \" + str(len(replay_buffer.storage)) )\n",
        "    actor_conv1_frozen = self.actor.conv1.weight.data.clone().detach()\n",
        "    actor_conv2_frozen = self.actor.conv2.weight.data.clone().detach()\n",
        "    actor_head_frozen = self.actor.head.weight.data.clone().detach()\n",
        "\n",
        "    for it in trange(iterations, desc = \"TD3 train function loop\"):\n",
        "      # print(\"Training iteration local: \" + str(it))\n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      # batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      batch_screen, batch_orientation, batch_dist_goal, batch_next_screen, batch_next_orientation, batch_next_dist_goal, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      \n",
        "      # Each element obtained from the replay_buffer is a np array\n",
        "      # Each element of the batch_states, batch_next_states is of the shape (1,1,size,size)\n",
        "      # Batch_states and batch_next_states have shapes (batch_size,1,1,size,size)\n",
        "      # What we want to feed to the conv networks is (batch_size, 1, size, size)\n",
        "      # torch.Tensor(batch_states) gives us a tensor of the shape (batch_size,1,1,size,size)\n",
        "      # Hence we squeeze out the first dimension ( zero'th dimension of size batch_size is intact )\n",
        "\n",
        "      # Each element of batch_rewards, batch_actions, batch_dones is a scalar\n",
        "      # We need to give a column vector to pytorch to compute losses\n",
        "      # Hence scalars go through a reshape(-1,1) ( eg: [1,2,3] to [[1],[2],[3]] )\n",
        "\n",
        "      screen = torch.Tensor(batch_screen).squeeze(1).to(self.device)\n",
        "      orientation = torch.Tensor(batch_orientation).view(-1,1).to(self.device) #Convert row vector to column vector eg: [1,2,3] to [[1],[2],[3]]\n",
        "      dist_goal = torch.Tensor(batch_dist_goal).view(-1,1).to(self.device) #Convert row vector to column vector eg: [1,2,3] to [[1],[2],[3]]\n",
        "      next_screen = torch.Tensor(batch_next_screen).squeeze(1).to(self.device) \n",
        "      next_orientation = torch.Tensor(batch_next_orientation).view(-1,1).to(self.device) #Convert row vector to column vector eg: [1,2,3] to [[1],[2],[3]]\n",
        "      next_dist_goal = torch.Tensor(batch_next_dist_goal).view(-1,1).to(self.device) #Convert row vector to column vector eg: [1,2,3] to [[1],[2],[3]]\n",
        "      action = torch.Tensor(batch_actions).view(-1,1).to(self.device) #Convert row vector to column vector eg: [1,2,3] to [[1],[2],[3]]\n",
        "      reward = torch.Tensor(batch_rewards).view(-1,1).to(self.device) #Convert row vector to column vector eg: [1,2,3] to [[1],[2],[3]]\n",
        "      done = torch.Tensor(batch_dones).view(-1,1).to(self.device) #Convert row vector to column vector eg: [1,2,3] to [[1],[2],[3]]\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_screen, next_orientation, next_dist_goal)\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise_distribution = torch.distributions.normal.Normal(0, policy_noise)\n",
        "      noise = noise_distribution.sample(torch.Size([batch_size])).clamp(-noise_clip,noise_clip).view(-1,1).to(self.device)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "        \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_screen, next_orientation, next_dist_goal, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(screen, orientation, dist_goal, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:        \n",
        "\n",
        "        actor_loss = -self.critic.Q1(screen, orientation, dist_goal, self.actor(screen, orientation, dist_goal)).mean()\n",
        "\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()        \n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "    \n",
        "    #Compute change in weights after the training iterations\n",
        "    actor_conv1_total_change = torch.sum( (torch.abs(self.actor.conv1.weight.data - actor_conv1_frozen) ) ).detach().cpu().numpy().flatten()[0]\n",
        "    actor_conv2_total_change = torch.sum( (torch.abs(self.actor.conv2.weight.data - actor_conv2_frozen) ) ).detach().cpu().numpy().flatten()[0]\n",
        "    actor_head_total_change = torch.sum( (torch.abs(self.actor.head.weight.data - actor_head_frozen) ) ).detach().cpu().numpy().flatten()[0]\n",
        "\n",
        "    print(\"Actor Conv 1 total change: \", actor_conv1_total_change )\n",
        "    print(\"Actor Conv 2 total change: \", actor_conv2_total_change )\n",
        "    print(\"Actor head total change: \", actor_head_total_change )\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP1Fz8VIuOXS",
        "colab_type": "text"
      },
      "source": [
        "# Generic helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zyg0Ovf4uOXU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mkdir(base, name):\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoGE7cekuOYG",
        "colab_type": "text"
      },
      "source": [
        "# Env declaration and variable setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgmD5x8DuOYT",
        "colab_type": "code",
        "outputId": "7086e589-7e73-40e6-af73-3c09980ac38a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#Setting params\n",
        "env_name = \"CityMap\"\n",
        "seed = 0 # Random seed number\n",
        "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 3e4 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "expl_noise = (0.2*np.pi)/(2.7*180) # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 100 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = (0.2*np.pi)/(2.7*180) # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated\n",
        "train_iterations = 1000 # Number of iterations to run the training cycle for each time an episide is over\n",
        "\n",
        "#File name for actor and critic model saved files\n",
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")\n",
        "\n",
        "#Create folder to save trained models\n",
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")\n",
        "\n",
        "#Create the environment\n",
        "citymap = Image.open(\"MASK1.png\")\n",
        "roadmask = Image.open(\"MASK1.png\").convert('L')\n",
        "car_image = Image.open(\"car_upright.png\")\n",
        "car_image_width, car_image_height = car_image.getbbox()[2:4]\n",
        "car_image_resized = car_image.resize( (int(car_image_width/2), int(car_image_height/2)) )\n",
        "\n",
        "work_dir = mkdir('exp', 'brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "monitor_car_pov_dir = mkdir(work_dir, 'monitor_car_pov')\n",
        "\n",
        "env = CityMap(citymap, roadmask, car_image_resized)\n",
        "\n",
        "# Set seeds and get get info to initiate classes\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_window_size\n",
        "action_dim = 1\n",
        "max_action = env.max_action\n",
        "\n",
        "#Create the policy network\n",
        "policy = TD3(state_dim, action_dim, max_action, device)\n",
        "\n",
        "#Create the experience replay buffer\n",
        "replay_buffer = ReplayBuffer(max_size = 5e5)\n",
        "\n",
        "#Initialise variables\n",
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "episode_timesteps = 1e3\n",
        "done = True\n",
        "t0 = time.time()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_CityMap_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zr6aJXXxZfsl",
        "colab_type": "code",
        "outputId": "731568ac-71f8-400c-8991-0b9587fe431c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF88u9Qfy7hE",
        "colab_type": "code",
        "outputId": "76c67e3c-1c9d-4fa1-e06b-e5412be46e0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "%%time\n",
        "#List for storing model evaluations\n",
        "evaluations = [evaluate_policy(policy, env)]\n",
        "evaluation_timesteps = [0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -3744.871599\n",
            "---------------------------------------\n",
            "CPU times: user 12 s, sys: 71.8 ms, total: 12.1 s\n",
            "Wall time: 12.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2zusv5ZuOZJ",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mi8H2dguOZW",
        "colab_type": "code",
        "outputId": "040d71ad-c0cd-4064-8d38-7fef326d9ffa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# We start the main loop over max_timesteps timesteps\n",
        "while total_timesteps < max_timesteps:\n",
        "  \n",
        "  # If the episode is done\n",
        "  if done:\n",
        "\n",
        "    # Complete the recording\n",
        "    # env.stats_recorder.save_complete()\n",
        "    # env.stats_recorder.done = True\n",
        "\n",
        "    # If we are not at the very beginning, we start the training process of the model\n",
        "    if total_timesteps != 0:\n",
        "      print(\"\\n\\nTotal Timesteps: {} Episode Num: {} Episode length: {} Reward: {}\".format(total_timesteps, episode_num, episode_timesteps, episode_reward))\n",
        "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "\n",
        "    # We evaluate the episode and we save the policy\n",
        "    if timesteps_since_eval >= eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      evaluations.append(evaluate_policy(policy, env))\n",
        "      evaluation_timesteps.append(total_timesteps)\n",
        "      policy.save(file_name, directory=\"./pytorch_models\")\n",
        "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "    \n",
        "    # When the training step is done, we reset the state of the environment\n",
        "    screen, orientation, dist_goal = env.reset()\n",
        "    \n",
        "    # Set the Done to False\n",
        "    done = False\n",
        "    \n",
        "    # Set rewards and episode timesteps to zero\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "  \n",
        "  # Before start_timesteps timesteps, we play random actions\n",
        "  if total_timesteps < start_timesteps:\n",
        "    action = env.action_space.sample()[0]\n",
        "  else: # After 10000 timesteps, we switch to the model\n",
        "    action = policy.select_action((screen, orientation, dist_goal))\n",
        "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
        "    if expl_noise != 0:\n",
        "      action = np.clip( (action + np.random.normal(0, expl_noise)), env.action_space.low, env.action_space.high)[0]\n",
        "      # np.clip returns an array. We need a scalar. Hence taking the first element\n",
        "  \n",
        "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "  (new_screen, new_orientation, new_dist_goal), reward, done, _ = env.step(action)\n",
        "\n",
        "  # We check if the episode is done\n",
        "  done_bool = 1 if episode_timesteps + 1 == env.max_episode_steps else float(done)\n",
        "  done = bool(done_bool)\n",
        "\n",
        "  # We increase the total reward\n",
        "  episode_reward += reward\n",
        "  \n",
        "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "  replay_buffer.add((screen, orientation, dist_goal, new_screen, new_orientation, new_dist_goal, action, reward, done_bool))\n",
        "\n",
        "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
        "  screen = new_screen\n",
        "  orientation = new_orientation\n",
        "  dist_goal = new_dist_goal\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "\n",
        "env.close()\n",
        "# Complete the recording\n",
        "# env.stats_recorder.save_complete()\n",
        "# env.stats_recorder.done = True\n",
        "\n",
        "# We add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy, env))\n",
        "evaluation_timesteps.append(total_timesteps)\n",
        "\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rTD3 train function loop:   0%|          | 0/400 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 400 Episode Num: 1 Episode length: 400 Reward: -366134.57628045866\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 400/400 [00:20<00:00, 19.07it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.0\n",
            "Actor Conv 2 total change:  0.0\n",
            "Actor head total change:  0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 2/1712 [00:00<01:25, 19.90it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 2112 Episode Num: 2 Episode length: 1712 Reward: -1176749.0687676207\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 1712/1712 [01:28<00:00, 19.32it/s]\n",
            "TD3 train function loop:   0%|          | 0/422 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.0\n",
            "Actor Conv 2 total change:  0.0\n",
            "Actor head total change:  0.0\n",
            "\n",
            "\n",
            "Total Timesteps: 2534 Episode Num: 3 Episode length: 422 Reward: -344016.1798729456\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 422/422 [00:21<00:00, 19.43it/s]\n",
            "TD3 train function loop:   0%|          | 0/515 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.24792907\n",
            "Actor Conv 2 total change:  1.2417815\n",
            "Actor head total change:  0.6235959\n",
            "\n",
            "\n",
            "Total Timesteps: 3049 Episode Num: 4 Episode length: 515 Reward: -461251.48870979523\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 515/515 [00:26<00:00, 19.25it/s]\n",
            "TD3 train function loop:   2%|▏         | 2/114 [00:00<00:05, 19.24it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.0\n",
            "Actor Conv 2 total change:  0.0\n",
            "Actor head total change:  0.0\n",
            "\n",
            "\n",
            "Total Timesteps: 3163 Episode Num: 5 Episode length: 114 Reward: 120.54651596660797\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 114/114 [00:06<00:00, 18.99it/s]\n",
            "TD3 train function loop:   0%|          | 0/514 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  1.4065971\n",
            "Actor Conv 2 total change:  8.4422245\n",
            "Actor head total change:  3.8583417\n",
            "\n",
            "\n",
            "Total Timesteps: 3677 Episode Num: 6 Episode length: 514 Reward: -63245.02363570762\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 514/514 [00:27<00:00, 18.95it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.04677741\n",
            "Actor Conv 2 total change:  0.42588714\n",
            "Actor head total change:  0.05265089\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 2/1124 [00:00<01:04, 17.38it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 4801 Episode Num: 7 Episode length: 1124 Reward: -822266.9352931727\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 1124/1124 [00:59<00:00, 18.94it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.063879885\n",
            "Actor Conv 2 total change:  0.72414315\n",
            "Actor head total change:  0.023935523\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 2/894 [00:00<00:45, 19.77it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 5695 Episode Num: 8 Episode length: 894 Reward: -683920.25565411\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 894/894 [00:46<00:00, 19.42it/s]\n",
            "TD3 train function loop:   0%|          | 0/414 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.013881625\n",
            "Actor Conv 2 total change:  0.24823493\n",
            "Actor head total change:  0.009312656\n",
            "\n",
            "\n",
            "Total Timesteps: 6109 Episode Num: 9 Episode length: 414 Reward: -60221.29750345129\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 414/414 [00:21<00:00, 19.47it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.0063196383\n",
            "Actor Conv 2 total change:  0.074301265\n",
            "Actor head total change:  0.002875032\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 2/2500 [00:00<02:20, 17.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 8609 Episode Num: 10 Episode length: 2500 Reward: -1922654.3198773994\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 2500/2500 [02:08<00:00, 19.41it/s]\n",
            "TD3 train function loop:   0%|          | 0/359 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.019346882\n",
            "Actor Conv 2 total change:  0.39805374\n",
            "Actor head total change:  0.013228838\n",
            "\n",
            "\n",
            "Total Timesteps: 8968 Episode Num: 11 Episode length: 359 Reward: -99386.66988767192\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 359/359 [00:18<00:00, 19.87it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.0017241846\n",
            "Actor Conv 2 total change:  0.029792033\n",
            "Actor head total change:  0.0012469612\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 3/2500 [00:00<02:06, 19.76it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 11468 Episode Num: 12 Episode length: 2500 Reward: -781459.2377450472\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 2500/2500 [02:03<00:00, 20.23it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.039442353\n",
            "Actor Conv 2 total change:  0.406153\n",
            "Actor head total change:  0.018178351\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 2/2500 [00:00<02:11, 18.98it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 13968 Episode Num: 13 Episode length: 2500 Reward: -2814.101729677771\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 2500/2500 [02:03<00:00, 20.21it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.11125992\n",
            "Actor Conv 2 total change:  1.1169283\n",
            "Actor head total change:  0.05115348\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 3/2339 [00:00<01:57, 19.87it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 16307 Episode Num: 14 Episode length: 2339 Reward: -616.8958527881116\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 2339/2339 [01:54<00:00, 20.49it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.35877228\n",
            "Actor Conv 2 total change:  4.287207\n",
            "Actor head total change:  0.94872946\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 2/2500 [00:00<02:08, 19.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 18807 Episode Num: 15 Episode length: 2500 Reward: -3365.0368253127963\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 2500/2500 [02:03<00:00, 20.30it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.060000718\n",
            "Actor Conv 2 total change:  0.5605683\n",
            "Actor head total change:  0.22336186\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 2/2500 [00:00<02:05, 19.90it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 21307 Episode Num: 16 Episode length: 2500 Reward: -1633.37145784318\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 2500/2500 [02:02<00:00, 20.48it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.052405525\n",
            "Actor Conv 2 total change:  0.5566656\n",
            "Actor head total change:  0.26526627\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 3/2500 [00:00<02:02, 20.34it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 23807 Episode Num: 17 Episode length: 2500 Reward: -69233.02934977767\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 2500/2500 [02:02<00:00, 20.47it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.058539018\n",
            "Actor Conv 2 total change:  0.65373504\n",
            "Actor head total change:  0.30145827\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 3/2500 [00:00<02:03, 20.24it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 26307 Episode Num: 18 Episode length: 2500 Reward: -69691.54078315408\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 2500/2500 [02:00<00:00, 20.75it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.0712157\n",
            "Actor Conv 2 total change:  0.6326574\n",
            "Actor head total change:  0.32741517\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 3/2500 [00:00<02:02, 20.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 28807 Episode Num: 19 Episode length: 2500 Reward: -8409.530584794873\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 2500/2500 [02:03<00:00, 20.27it/s]\n",
            "TD3 train function loop:  14%|█▍        | 3/21 [00:00<00:00, 20.33it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.0513215\n",
            "Actor Conv 2 total change:  0.3655808\n",
            "Actor head total change:  0.16750503\n",
            "\n",
            "\n",
            "Total Timesteps: 28828 Episode Num: 20 Episode length: 21 Reward: 103.88291457482815\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 21/21 [00:01<00:00, 20.12it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.0018951343\n",
            "Actor Conv 2 total change:  0.021016214\n",
            "Actor head total change:  0.0010843053\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 3/2500 [00:00<02:07, 19.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 31328 Episode Num: 21 Episode length: 2500 Reward: -8567.097836918172\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 2500/2500 [02:00<00:00, 20.70it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.075380534\n",
            "Actor Conv 2 total change:  1.0231817\n",
            "Actor head total change:  0.21351233\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -48615.379397\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 2/2500 [00:00<02:06, 19.81it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 33828 Episode Num: 22 Episode length: 2500 Reward: -6345.4728217920365\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 2500/2500 [02:04<00:00, 20.07it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.049798656\n",
            "Actor Conv 2 total change:  0.53530234\n",
            "Actor head total change:  0.11170448\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 3/2500 [00:00<02:02, 20.40it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 36328 Episode Num: 23 Episode length: 2500 Reward: -3244.024152959196\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 2500/2500 [02:04<00:00, 20.09it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.07387913\n",
            "Actor Conv 2 total change:  0.59979\n",
            "Actor head total change:  0.13088226\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 3/2500 [00:00<02:05, 19.90it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 38828 Episode Num: 24 Episode length: 2500 Reward: -5419.446009164249\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 2500/2500 [02:03<00:00, 20.32it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.04319266\n",
            "Actor Conv 2 total change:  0.49916422\n",
            "Actor head total change:  0.09623495\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 3/2500 [00:00<02:03, 20.15it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 41328 Episode Num: 25 Episode length: 2500 Reward: -3352.426910418221\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 2500/2500 [02:03<00:00, 20.27it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.046597987\n",
            "Actor Conv 2 total change:  0.5889816\n",
            "Actor head total change:  0.16483805\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 2/2500 [00:00<02:05, 19.89it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 43828 Episode Num: 26 Episode length: 2500 Reward: -5370.318309860621\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 2500/2500 [02:00<00:00, 20.82it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.07959764\n",
            "Actor Conv 2 total change:  0.5784153\n",
            "Actor head total change:  0.12115194\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 3/2500 [00:00<02:04, 20.07it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 46328 Episode Num: 27 Episode length: 2500 Reward: -16237.90184628305\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 2500/2500 [02:04<00:00, 20.06it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.19014587\n",
            "Actor Conv 2 total change:  0.8232739\n",
            "Actor head total change:  0.24081545\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 2/2500 [00:00<02:11, 18.99it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 48828 Episode Num: 28 Episode length: 2500 Reward: -177475.93154695723\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 2500/2500 [02:02<00:00, 20.49it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.101592794\n",
            "Actor Conv 2 total change:  0.4954302\n",
            "Actor head total change:  0.13609615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 2/2500 [00:00<02:06, 19.74it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 51328 Episode Num: 29 Episode length: 2500 Reward: -30525.972003520754\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 2500/2500 [02:03<00:00, 20.26it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  2.727611e-07\n",
            "Actor Conv 2 total change:  8.333009e-07\n",
            "Actor head total change:  1.5541445e-08\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 3/2500 [00:00<02:04, 20.05it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 53828 Episode Num: 30 Episode length: 2500 Reward: -96544.16640606019\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 2500/2500 [02:03<00:00, 20.23it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.0\n",
            "Actor Conv 2 total change:  0.0\n",
            "Actor head total change:  0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 2/2500 [00:00<02:13, 18.67it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 56328 Episode Num: 31 Episode length: 2500 Reward: -8330.11008208983\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 2500/2500 [02:02<00:00, 20.33it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.0\n",
            "Actor Conv 2 total change:  0.0\n",
            "Actor head total change:  0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 3/2500 [00:00<02:02, 20.43it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 58828 Episode Num: 32 Episode length: 2500 Reward: -7783.4488414239095\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 2500/2500 [02:03<00:00, 20.32it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.0\n",
            "Actor Conv 2 total change:  0.0\n",
            "Actor head total change:  0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 2/2500 [00:00<02:06, 19.71it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 61328 Episode Num: 33 Episode length: 2500 Reward: -6251.377584854878\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 2500/2500 [02:03<00:00, 20.22it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.06829145\n",
            "Actor Conv 2 total change:  0.33626986\n",
            "Actor head total change:  0.09173908\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -3160.452688\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 3/2500 [00:00<02:04, 20.02it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 63828 Episode Num: 34 Episode length: 2500 Reward: -10451.895720049035\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 2500/2500 [02:03<00:00, 20.23it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.001755324\n",
            "Actor Conv 2 total change:  0.008636873\n",
            "Actor head total change:  0.0023593744\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 3/2500 [00:00<02:02, 20.44it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 66328 Episode Num: 35 Episode length: 2500 Reward: -3234.6241287929947\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 2500/2500 [02:01<00:00, 20.50it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.0\n",
            "Actor Conv 2 total change:  0.0\n",
            "Actor head total change:  0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 2/2500 [00:00<02:15, 18.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 68828 Episode Num: 36 Episode length: 2500 Reward: -2426.0091989533935\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 2500/2500 [02:06<00:00, 19.73it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.0\n",
            "Actor Conv 2 total change:  0.0\n",
            "Actor head total change:  0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 2/2500 [00:00<02:14, 18.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 71328 Episode Num: 37 Episode length: 2500 Reward: -8487.794721339409\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 2500/2500 [02:05<00:00, 19.84it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.0\n",
            "Actor Conv 2 total change:  0.0\n",
            "Actor head total change:  0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 2/2500 [00:00<02:21, 17.69it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 73828 Episode Num: 38 Episode length: 2500 Reward: -2467.997757962223\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 2500/2500 [02:08<00:00, 19.48it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.0\n",
            "Actor Conv 2 total change:  0.0\n",
            "Actor head total change:  0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 2/2500 [00:00<02:10, 19.20it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 76328 Episode Num: 39 Episode length: 2500 Reward: -8095.290191027403\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 2500/2500 [02:12<00:00, 18.85it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.0\n",
            "Actor Conv 2 total change:  0.0\n",
            "Actor head total change:  0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 2/2500 [00:00<02:19, 17.95it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 78828 Episode Num: 40 Episode length: 2500 Reward: -1973.323389823408\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 2500/2500 [02:10<00:00, 19.18it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.0\n",
            "Actor Conv 2 total change:  0.0\n",
            "Actor head total change:  0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 2/2500 [00:00<02:17, 18.15it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 81328 Episode Num: 41 Episode length: 2500 Reward: -4020.089899393108\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 2500/2500 [02:11<00:00, 19.07it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.0\n",
            "Actor Conv 2 total change:  0.0\n",
            "Actor head total change:  0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 2/2500 [00:00<02:12, 18.80it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 83828 Episode Num: 42 Episode length: 2500 Reward: -5032.813257271639\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop: 100%|██████████| 2500/2500 [02:10<00:00, 19.15it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actor Conv 1 total change:  0.0\n",
            "Actor Conv 2 total change:  0.0\n",
            "Actor head total change:  0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:   0%|          | 2/2500 [00:00<02:13, 18.73it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Timesteps: 86328 Episode Num: 43 Episode length: 2500 Reward: -17828.51091546916\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TD3 train function loop:  21%|██        | 514/2500 [00:27<01:44, 18.97it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-6c3515b1cefa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtotal_timesteps\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\nTotal Timesteps: {} Episode Num: {} Episode length: {} Reward: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m       \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_clip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# We evaluate the episode and we save the policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-cad14e97e17f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, replay_buffer, iterations, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\u001b[0m\n\u001b[1;32m    170\u001b[0m       \u001b[0;31m# Hence scalars go through a reshape(-1,1) ( eg: [1,2,3] to [[1],[2],[3]] )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m       \u001b[0mscreen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_screen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m       \u001b[0morientation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_orientation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Convert row vector to column vector eg: [1,2,3] to [[1],[2],[3]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mdist_goal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_dist_goal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Convert row vector to column vector eg: [1,2,3] to [[1],[2],[3]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnQFT1FwuOZ2",
        "colab_type": "text"
      },
      "source": [
        "# Plot evaluation score over time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owbXJ_PVuOZ5",
        "colab_type": "code",
        "outputId": "8c4587f9-0eeb-4180-f0c5-ac8643386e49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        }
      },
      "source": [
        "fig, ax = plt.subplots(1,1, figsize = (12,9))\n",
        "ax.plot( evaluation_timesteps, evaluations )\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_xlabel('Timesteps')\n",
        "ax.set_title('Agent score vs iterations');"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x648 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvjGbqqk2Hf-",
        "colab_type": "text"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrzsQgYV2IY-",
        "colab_type": "code",
        "outputId": "e7c1a7e8-4976-49b7-e585-253b49df1dab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "policy_inference = TD3(state_dim, action_dim, max_action, device)\n",
        "policy_inference.load(file_name, './pytorch_models/')\n",
        "\n",
        "env_inference = CityMap(citymap, roadmask, car_image_resized, render_pov = 'map')\n",
        "env_inference = wrappers.Monitor(env_inference, monitor_dir, force = True, video_callable=lambda episode_id: True)\n",
        "\n",
        "avg_reward_inference = evaluate_policy(policy_inference, env_inference, eval_episodes=3)\n",
        "print(\"Iter 1 avg reward: \", avg_reward_inference)\n",
        "\n",
        "# Wrapup recording\n",
        "env_inference.close()\n",
        "env_inference.stats_recorder.save_complete()\n",
        "env_inference.stats_recorder.done = True\n",
        "\n",
        "env_inference = CityMap(citymap, roadmask, car_image_resized, render_pov = 'car')\n",
        "env_inference = wrappers.Monitor(env_inference, monitor_car_pov_dir, force = True, video_callable=lambda episode_id: True)\n",
        "\n",
        "avg_reward_inference = evaluate_policy(policy_inference, env_inference, eval_episodes=3)\n",
        "print(\"Iter 2 avg reward: \", avg_reward_inference)\n",
        "\n",
        "# Wrapup recording\n",
        "env_inference.close()\n",
        "env_inference.stats_recorder.save_complete()\n",
        "env_inference.stats_recorder.done = True"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Trying to monitor an environment which has no 'spec' set. This usually means you did not create it via 'gym.make', and is recommended only for advanced users.\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -20577.060258\n",
            "---------------------------------------\n",
            "Iter 1 avg reward:  -20577.060258386362\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Trying to monitor an environment which has no 'spec' set. This usually means you did not create it via 'gym.make', and is recommended only for advanced users.\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -19788.749159\n",
            "---------------------------------------\n",
            "Iter 2 avg reward:  -19788.7491594367\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}